{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "shakespear.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGJo_lUBxwts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtoZpv1wxQ70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5Cz78CTxQ78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Open data containing Shakespear's poetry\n",
        "with open('shakespear.txt') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1aOVqhpxQ8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f0b38c5b-a26f-4c79-eeb2-02efaacaf20a"
      },
      "source": [
        "# Sample of the data\n",
        "text[:113]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Cegc4jxQ8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoding the text and map each character into number and vice-versa\n",
        "\n",
        "# We create 2 dctonaries\n",
        "# int2char: maps number into characters\n",
        "# char2int: maps characters into numbers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii, ch in int2char.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqQ4sDU8ySaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b26953dd-0bd9-4b4c-bb2c-9fbc4e693e0a"
      },
      "source": [
        "print(chars)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('k', 'S', '5', 'Y', '8', 'n', ':', 'm', 't', '?', 'j', '1', '9', '[', 'Z', 'e', 'N', 'o', ' ', '4', '0', 'Q', '&', 'H', 'f', 'u', '2', 'P', 'L', 'M', 'q', 'c', 'p', 'W', ')', 'T', 's', 'r', 'w', '|', 'D', 'J', 'X', 'F', 'A', 'R', 'B', 'C', 'z', 'i', 'a', 'U', '\"', 'y', 'v', '_', '\\n', 'V', '3', '>', \"'\", '`', '(', ';', '<', 'h', '6', 'g', 'G', 'l', 'b', 'K', 'I', 'd', 'O', '-', 'x', '}', '7', '!', ',', ']', 'E', '.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFUYvDALxQ8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the text\n",
        "encode = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s3ztHSGxQ8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2cfe8f8-e1f7-45fc-de56-f36e93c2a0a4"
      },
      "source": [
        "encode[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dQ4BxyoxQ8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc1340f8-f457-4428-eea8-dc8c67f8cfb2"
      },
      "source": [
        "print(char2int['\\n'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JULOpeBdxQ8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining method to encode one hot labels\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    # Initilize the one hot encoded array\n",
        "    one_hot = np.zeros( (np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "    \n",
        "    # Finally reshape it to get back the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFoqYo8cxQ8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining method to make mini-batches for training\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    \n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceAasPrkxQ8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check gpu is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "#train_on_gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DGMgUAUxQ88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declaring the model\n",
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=4,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        #define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        #define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        \n",
        "        #define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        #get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        #pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        #put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0J2n3-HxQ8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declaring the train method\n",
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHgUSkI3xQ9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b44fd3ae-a618-4d41-e645-99bfde62d692"
      },
      "source": [
        "# Define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "\n",
        "# Declaring the hyperparameters\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 25 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encode, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(84, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=84, bias=True)\n",
            ")\n",
            "Epoch: 1/25... Step: 100... Loss: 3.1870... Val Loss: 3.1209\n",
            "Epoch: 1/25... Step: 200... Loss: 2.6308... Val Loss: 2.5452\n",
            "Epoch: 2/25... Step: 300... Loss: 2.2705... Val Loss: 2.1459\n",
            "Epoch: 2/25... Step: 400... Loss: 2.0948... Val Loss: 1.9773\n",
            "Epoch: 2/25... Step: 500... Loss: 1.9114... Val Loss: 1.8583\n",
            "Epoch: 3/25... Step: 600... Loss: 1.8315... Val Loss: 1.7722\n",
            "Epoch: 3/25... Step: 700... Loss: 1.7667... Val Loss: 1.7003\n",
            "Epoch: 3/25... Step: 800... Loss: 1.7734... Val Loss: 1.6405\n",
            "Epoch: 4/25... Step: 900... Loss: 1.6770... Val Loss: 1.6031\n",
            "Epoch: 4/25... Step: 1000... Loss: 1.6212... Val Loss: 1.5696\n",
            "Epoch: 4/25... Step: 1100... Loss: 1.6208... Val Loss: 1.5379\n",
            "Epoch: 5/25... Step: 1200... Loss: 1.5793... Val Loss: 1.5183\n",
            "Epoch: 5/25... Step: 1300... Loss: 1.5419... Val Loss: 1.4924\n",
            "Epoch: 5/25... Step: 1400... Loss: 1.5392... Val Loss: 1.4700\n",
            "Epoch: 6/25... Step: 1500... Loss: 1.5268... Val Loss: 1.4507\n",
            "Epoch: 6/25... Step: 1600... Loss: 1.5162... Val Loss: 1.4352\n",
            "Epoch: 7/25... Step: 1700... Loss: 1.4262... Val Loss: 1.4169\n",
            "Epoch: 7/25... Step: 1800... Loss: 1.4504... Val Loss: 1.4086\n",
            "Epoch: 7/25... Step: 1900... Loss: 1.4208... Val Loss: 1.3992\n",
            "Epoch: 8/25... Step: 2000... Loss: 1.4282... Val Loss: 1.3892\n",
            "Epoch: 8/25... Step: 2100... Loss: 1.4043... Val Loss: 1.3778\n",
            "Epoch: 8/25... Step: 2200... Loss: 1.3815... Val Loss: 1.3623\n",
            "Epoch: 9/25... Step: 2300... Loss: 1.4443... Val Loss: 1.3743\n",
            "Epoch: 9/25... Step: 2400... Loss: 1.3228... Val Loss: 1.3507\n",
            "Epoch: 9/25... Step: 2500... Loss: 1.3829... Val Loss: 1.3441\n",
            "Epoch: 10/25... Step: 2600... Loss: 1.3580... Val Loss: 1.3402\n",
            "Epoch: 10/25... Step: 2700... Loss: 1.3128... Val Loss: 1.3325\n",
            "Epoch: 10/25... Step: 2800... Loss: 1.2900... Val Loss: 1.3233\n",
            "Epoch: 11/25... Step: 2900... Loss: 1.3220... Val Loss: 1.3203\n",
            "Epoch: 11/25... Step: 3000... Loss: 1.3167... Val Loss: 1.3151\n",
            "Epoch: 12/25... Step: 3100... Loss: 1.3028... Val Loss: 1.3073\n",
            "Epoch: 12/25... Step: 3200... Loss: 1.2664... Val Loss: 1.3009\n",
            "Epoch: 12/25... Step: 3300... Loss: 1.2589... Val Loss: 1.3041\n",
            "Epoch: 13/25... Step: 3400... Loss: 1.2629... Val Loss: 1.2941\n",
            "Epoch: 13/25... Step: 3500... Loss: 1.2926... Val Loss: 1.2914\n",
            "Epoch: 13/25... Step: 3600... Loss: 1.2307... Val Loss: 1.2861\n",
            "Epoch: 14/25... Step: 3700... Loss: 1.2698... Val Loss: 1.2805\n",
            "Epoch: 14/25... Step: 3800... Loss: 1.2886... Val Loss: 1.2790\n",
            "Epoch: 14/25... Step: 3900... Loss: 1.2722... Val Loss: 1.2794\n",
            "Epoch: 15/25... Step: 4000... Loss: 1.2972... Val Loss: 1.2761\n",
            "Epoch: 15/25... Step: 4100... Loss: 1.2415... Val Loss: 1.2719\n",
            "Epoch: 15/25... Step: 4200... Loss: 1.2628... Val Loss: 1.2675\n",
            "Epoch: 16/25... Step: 4300... Loss: 1.2506... Val Loss: 1.2646\n",
            "Epoch: 16/25... Step: 4400... Loss: 1.2242... Val Loss: 1.2638\n",
            "Epoch: 17/25... Step: 4500... Loss: 1.2322... Val Loss: 1.2635\n",
            "Epoch: 17/25... Step: 4600... Loss: 1.2199... Val Loss: 1.2563\n",
            "Epoch: 17/25... Step: 4700... Loss: 1.2247... Val Loss: 1.2602\n",
            "Epoch: 18/25... Step: 4800... Loss: 1.2334... Val Loss: 1.2512\n",
            "Epoch: 18/25... Step: 4900... Loss: 1.2230... Val Loss: 1.2522\n",
            "Epoch: 18/25... Step: 5000... Loss: 1.2161... Val Loss: 1.2548\n",
            "Epoch: 19/25... Step: 5100... Loss: 1.2492... Val Loss: 1.2501\n",
            "Epoch: 19/25... Step: 5200... Loss: 1.2195... Val Loss: 1.2499\n",
            "Epoch: 19/25... Step: 5300... Loss: 1.2569... Val Loss: 1.2457\n",
            "Epoch: 20/25... Step: 5400... Loss: 1.2089... Val Loss: 1.2434\n",
            "Epoch: 20/25... Step: 5500... Loss: 1.2025... Val Loss: 1.2455\n",
            "Epoch: 20/25... Step: 5600... Loss: 1.2229... Val Loss: 1.2458\n",
            "Epoch: 21/25... Step: 5700... Loss: 1.1561... Val Loss: 1.2416\n",
            "Epoch: 21/25... Step: 5800... Loss: 1.2136... Val Loss: 1.2403\n",
            "Epoch: 21/25... Step: 5900... Loss: 1.2244... Val Loss: 1.2351\n",
            "Epoch: 22/25... Step: 6000... Loss: 1.2100... Val Loss: 1.2398\n",
            "Epoch: 22/25... Step: 6100... Loss: 1.2094... Val Loss: 1.2424\n",
            "Epoch: 23/25... Step: 6200... Loss: 1.1909... Val Loss: 1.2373\n",
            "Epoch: 23/25... Step: 6300... Loss: 1.1811... Val Loss: 1.2402\n",
            "Epoch: 23/25... Step: 6400... Loss: 1.2207... Val Loss: 1.2404\n",
            "Epoch: 24/25... Step: 6500... Loss: 1.1806... Val Loss: 1.2368\n",
            "Epoch: 24/25... Step: 6600... Loss: 1.1847... Val Loss: 1.2388\n",
            "Epoch: 24/25... Step: 6700... Loss: 1.1483... Val Loss: 1.2370\n",
            "Epoch: 25/25... Step: 6800... Loss: 1.1817... Val Loss: 1.2360\n",
            "Epoch: 25/25... Step: 6900... Loss: 1.1407... Val Loss: 1.2365\n",
            "Epoch: 25/25... Step: 7000... Loss: 1.1835... Val Loss: 1.2372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5z2iYN4zdLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a method to generate the next character\n",
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvbPs_pLzfN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declaring a method to generate new text\n",
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb7cwCWRzjly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "f8b01ed2-1c31-4d67-a6e2-b96737932ee6"
      },
      "source": [
        "# Generating new text\n",
        "print(sample(net, 1000, prime='H', top_k=5))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hy sere trind\n",
            "    As a that him hits that he word or me, to blow,\n",
            "                       \n",
            "                                                    Eneen I with thy more tone wises be the mast\n",
            " \n",
            "                                                   Exeunt PALESAR, Angeare a doors]              \"   \n",
            "    And the will on that the Kert and thom sore,\n",
            "                              Enent the CORES, INDIAND, INLEND OR ANTOR OF OLDOSTALE Th          THe talled are the starl, the well.\n",
            "    I thou to but the mean of that so stees thou a dore\n",
            "    ar we him is as the wiss bat in the pores an this and\n",
            "    then thin here a thee sele the pontens thee weal me mare thee mastel of to the\n",
            "    beasites be wour hiss outhing and the best in sing.\n",
            "  PORIUS. I ame ald the more weer siss thee thou speit.\n",
            "  PURIAN. His somenen sime to but thou and a mast.\n",
            "    That with and as such it hene and bound his bare.\n",
            "    Ther wise all to my sule, thee to that shell ane my buld\n",
            "    And hine of me so alle wath the manter to be sees \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLfy3Zhl1GsM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "6f6054de-3bfb-413f-8fe8-74d95adacb1f"
      },
      "source": [
        "# Generating new text\n",
        "print(sample(net, 500, prime='I', top_k=5))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IDAULES OF SOLENET, and SOMERSET,\n",
            "                                       Exit SHELIND and LANDES\n",
            "\n",
            "  SIR JUHN. A weary save your pleasuse and marry on,\n",
            "    We was a must shall not be mercy'd to mischance,\n",
            "    For the minister to the company of my\n",
            "    That I have break a praises and such torce of man,\n",
            "    What it then to survey heaven to speak a prepent\n",
            "    And then with survey and such arts at these speess\n",
            "    The sell of the will should heard all some speak,\n",
            "    Wherein a conforn that the wish's t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks-WDN65knU9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "7cb22947-7eac-46b7-dedb-3aebaf281277"
      },
      "source": [
        "# Generating new text\n",
        "print(sample(net, 500, prime='I Like', top_k=5))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I Like trees,\n",
            "    That I will stay well.\n",
            "  BATTH. Take my lord, I will but most must be my son;\n",
            "    To make thy soul a whell there is to make.\n",
            "    The matter will not serve\n",
            "    The patch of her thrist, to the whom hath say\n",
            "    Antent them; fir, the warlike and a sort\n",
            "    Where his own life is so tried with me.\n",
            "    Words to your Grace so shall I die as still\n",
            "    With trade of steep against him, who impain'd him\n",
            "    That take her strangles thou and stead of speed;\n",
            "    Ther thou art all my peace.\n",
            "  BASSAN\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}